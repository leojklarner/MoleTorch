{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43bb15ee",
   "metadata": {},
   "source": [
    "## Bayesian Optimisation Over Molecules ##\n",
    "\n",
    "An example notebook for Bayesian optimisation on a molecular dataset using a Tanimoto fingerprint kernel and the photoswitch dataset\n",
    "\n",
    "Paper: https://pubs.rsc.org/en/content/articlelanding/2022/sc/d2sc04306h\n",
    "\n",
    "Code: https://github.com/Ryan-Rhys/The-Photoswitch-Dataset\n",
    "\n",
    "A key aspect of this Bayesian optimisation loop is that the queried molecules, $\\mathbf{x^*}$. are drawn from a discrete set of heldout molecules, $\\mathcal{D}_{\\text{heldout}}$. Such situations may arise in virtual screening campaigns where one wishes to select a molecule for synthesis from a virtual library. In this case the acquisition function is evaluated on a discrete set and the maximum of the set is taken as the proposed candidate at each iteration of Bayesian optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6d4bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# To import from the gprotorch package\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Turn off Graphein warnings\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from botorch import fit_gpytorch_model\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from botorch.exceptions import BadInitialCandidatesWarning\n",
    "from botorch.models.gp_regression import SingleTaskGP\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.kernels import ScaleKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gauche.dataloader import DataLoaderMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95834d5e",
   "metadata": {},
   "source": [
    "We define our model. See\n",
    "\n",
    "https://docs.gpytorch.ai/en/latest/examples/01_Exact_GPs/Simple_GP_Regression.html\n",
    "\n",
    "for further examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c67cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gauche.kernels.fingerprint_kernels.tanimoto_kernel import TanimotoKernel\n",
    "\n",
    "# We define our custom GP surrogate model using the Tanimoto kernel\n",
    "\n",
    "class TanimotoGP(SingleTaskGP):\n",
    "\n",
    "    def __init__(self, train_X, train_Y):\n",
    "        super().__init__(train_X, train_Y, GaussianLikelihood())\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(base_kernel=TanimotoKernel())\n",
    "        self.to(train_X)  # make sure we're on the right device/dtype\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff61ddd4",
   "metadata": {},
   "source": [
    "We define helper functions for the Bayesian optimisation loop. In particular the acquisition function optimisation procedure is framed so as to take the maximum over a discrete set of heldout molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(train_x, train_obj, state_dict=None):\n",
    "    \"\"\"\n",
    "    Initialise model and loss function.\n",
    "\n",
    "    Args:\n",
    "        train_x: tensor of inputs\n",
    "        train_obj: tensor of outputs\n",
    "        state_dict: current state dict used to speed up fitting\n",
    "\n",
    "    Returns: mll object, model object\n",
    "    \"\"\"\n",
    "\n",
    "    # define model for objective\n",
    "    model = TanimotoGP(train_x, train_obj).to(train_x)\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    # load state dict if it is passed\n",
    "    if state_dict is not None:\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    return mll, model\n",
    "\n",
    "\n",
    "def optimize_acqf_and_get_observation(acq_func, heldout_inputs, heldout_outputs):\n",
    "    \"\"\"\n",
    "    Optimizes the acquisition function, and returns a new candidate and an observation.\n",
    "\n",
    "    Args:\n",
    "        acq_func: Object representing the acquisition function\n",
    "        heldout_points: Tensor of heldout points\n",
    "\n",
    "    Returns: new_x, new_obj\n",
    "    \"\"\"\n",
    "\n",
    "    # Loop over the discrete set of points to evaluate the acquisition function at.\n",
    "    acq_vals = []\n",
    "    for i in range(len(heldout_outputs)):\n",
    "        acq_vals.append(acq_func(heldout_inputs[i].unsqueeze(-2)))  # use unsqueeze to append batch dimension\n",
    "\n",
    "    # observe new values\n",
    "    acq_vals = torch.tensor(acq_vals)\n",
    "    best_idx = torch.argmax(acq_vals)\n",
    "    new_x = heldout_inputs[best_idx].unsqueeze(-2)  # add batch dimension\n",
    "    new_obj = heldout_outputs[best_idx].unsqueeze(-1)  # add output dimension\n",
    "\n",
    "    # Delete the selected input and value from the heldout set.\n",
    "    heldout_inputs = torch.cat((heldout_inputs[:best_idx], heldout_inputs[best_idx+1:]), axis=0)\n",
    "    heldout_outputs = torch.cat((heldout_outputs[:best_idx], heldout_outputs[best_idx+1:]), axis=0)\n",
    "\n",
    "    return new_x, new_obj, heldout_inputs, heldout_outputs\n",
    "\n",
    "\n",
    "def update_random_observations(best_random, heldout_inputs, heldout_outputs):\n",
    "    \"\"\"\n",
    "    Simulates a random policy by taking a the current list of best values observed randomly,\n",
    "    drawing a new random point from the heldout set, observing its value, and updating the list.\n",
    "\n",
    "    Args:\n",
    "        best_random: List of best random values observed so far\n",
    "        heldout_inputs: Tensor of inputs\n",
    "        heldout_outputs: Tensor of output values\n",
    "\n",
    "    Returns: best_random, float specifying the objective function value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Take a random sample by permuting the indices and selecting the first element.\n",
    "    index = torch.randperm(len(heldout_outputs))[0]\n",
    "    next_random_best = heldout_outputs[index]\n",
    "    best_random.append(max(best_random[-1], next_random_best))\n",
    "\n",
    "    # Delete the selected input and value from the heldout set.\n",
    "    heldout_inputs = torch.cat((heldout_inputs[:index], heldout_inputs[index+1:]), axis=0)\n",
    "    heldout_outputs = torch.cat((heldout_outputs[:index], heldout_outputs[index+1:]), axis=0)\n",
    "\n",
    "    return best_random, heldout_inputs, heldout_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7cb87",
   "metadata": {},
   "source": [
    "Run the Bayesian optimisation loop, comparing the analytic (sequential) expected improvement acquisition funciton with a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian optimisation experiment parameters, number of random trials, split size, batch size\n",
    "# and number of iterations of Bayesian optimisation.\n",
    "\n",
    "N_TRIALS = 20\n",
    "holdout_set_size = 0.95\n",
    "N_ITERS = 20\n",
    "verbose = False\n",
    "\n",
    "# Load the Photoswitch dataset\n",
    "loader = DataLoaderMP()\n",
    "loader.load_benchmark(\"Photoswitch\", \"../data/property_prediction/photoswitches.csv\")\n",
    "\n",
    "# We use the fragprints representations (a concatenation of Morgan fingerprints and RDKit fragment features)\n",
    "loader.featurize('fragprints')\n",
    "X = loader.features\n",
    "y = loader.labels\n",
    "\n",
    "warnings.filterwarnings('ignore', category=BadInitialCandidatesWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "best_observed_all_ei, best_random_all = [], []\n",
    "\n",
    "# average over multiple random trials (each trial splits the initial training set for the GP in a random manner)\n",
    "for trial in range(1, N_TRIALS + 1):\n",
    "\n",
    "    print(f\"\\nTrial {trial:>2} of {N_TRIALS} \", end=\"\")\n",
    "    best_observed_ei, best_random = [], []\n",
    "\n",
    "    # Generate initial training data and initialize model\n",
    "    train_x_ei, heldout_x_ei, train_y_ei, heldout_y_ei = train_test_split(X, y, test_size=holdout_set_size, random_state=trial)\n",
    "    best_observed_value_ei = torch.tensor(np.max(train_y_ei))\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors and flatten the label vectors\n",
    "    train_x_ei = torch.tensor(train_x_ei.astype(np.float64))\n",
    "    heldout_x_ei = torch.tensor(heldout_x_ei.astype(np.float64))\n",
    "    train_y_ei = torch.tensor(train_y_ei)\n",
    "    heldout_y_ei = torch.tensor(heldout_y_ei)\n",
    "\n",
    "    # The initial heldout set is the same for random search\n",
    "    heldout_x_random = heldout_x_ei\n",
    "    heldout_y_random = heldout_y_ei\n",
    "\n",
    "    mll_ei, model_ei = initialize_model(train_x_ei, train_y_ei)\n",
    "\n",
    "    best_observed_ei.append(best_observed_value_ei)\n",
    "    best_random.append(best_observed_value_ei)\n",
    "\n",
    "    # run N_ITERS rounds of BayesOpt after the initial random batch\n",
    "    for iteration in range(1, N_ITERS + 1):\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # fit the model\n",
    "        fit_gpytorch_model(mll_ei)\n",
    "\n",
    "        # Use analytic acquisition function for batch size of 1.\n",
    "        EI = ExpectedImprovement(model=model_ei, best_f=(train_y_ei.to(train_y_ei)).max())\n",
    "\n",
    "        new_x_ei, new_obj_ei, heldout_x_ei, heldout_y_ei = optimize_acqf_and_get_observation(EI,\n",
    "                                                                                             heldout_x_ei,\n",
    "                                                                                             heldout_y_ei)\n",
    "\n",
    "        # update training points\n",
    "        train_x_ei = torch.cat([train_x_ei, new_x_ei])\n",
    "        train_y_ei = torch.cat([train_y_ei, new_obj_ei])\n",
    "\n",
    "        # update random search progress\n",
    "        best_random, heldout_x_random, heldout_y_random = update_random_observations(best_random,\n",
    "                                                                                     heldout_inputs=heldout_x_random,\n",
    "                                                                                     heldout_outputs=heldout_y_random)\n",
    "        best_value_ei = torch.max(new_obj_ei, best_observed_ei[-1])\n",
    "        best_observed_ei.append(best_value_ei)\n",
    "\n",
    "        # reinitialise the model so it is ready for fitting on the next iteration\n",
    "        # use the current state dict to speed up fitting\n",
    "        mll_ei, model_ei = initialize_model(\n",
    "            train_x_ei,\n",
    "            train_y_ei,\n",
    "            model_ei.state_dict(),\n",
    "        )\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"\\nBatch {iteration:>2}: best_value (random, qEI) = \"\n",
    "                f\"({max(best_random):>4.2f}, {best_value_ei:>4.2f}), \"\n",
    "                f\"time = {t1 - t0:>4.2f}.\", end=\"\"\n",
    "            )\n",
    "        else:\n",
    "            print(\".\", end=\"\")\n",
    "\n",
    "    best_observed_all_ei.append(best_observed_ei)\n",
    "    best_random_all.append(best_random)\n",
    "\n",
    "# Define a confience interval function for plotting.\n",
    "def ci(y):\n",
    "    return 1.96 * y.std(axis=0) / np.sqrt(N_TRIALS)\n",
    "\n",
    "iters = np.arange(N_ITERS + 1)\n",
    "y_ei = np.asarray(best_observed_all_ei)\n",
    "y_rnd = np.asarray(best_random_all)\n",
    "\n",
    "y_rnd_mean = y_rnd.mean(axis=0)\n",
    "y_ei_mean = y_ei.mean(axis=0)\n",
    "y_rnd_std = y_rnd.std(axis=0)\n",
    "y_ei_std = y_ei.std(axis=0)\n",
    "\n",
    "lower_rnd = y_rnd_mean - y_rnd_std\n",
    "upper_rnd = y_rnd_mean + y_rnd_std\n",
    "lower_ei = y_ei_mean - y_ei_std\n",
    "upper_ei = y_ei_mean + y_ei_std\n",
    "\n",
    "plt.plot(iters, y_rnd_mean, label='Random')\n",
    "plt.fill_between(iters, lower_rnd, upper_rnd, alpha=0.2)\n",
    "plt.plot(iters, y_ei_mean, label='EI')\n",
    "plt.fill_between(iters, lower_ei, upper_ei, alpha=0.2)\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Best Objective Value')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xticks(list(np.arange(1, 21)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35bee4",
   "metadata": {},
   "source": [
    "EI outperforms random search in terms of selecting molecules with high E isomer pi-pi* transition wavelength! It should be noted that the true objective for photoswitch optimisation would consider all transition wavelengths as well as the thermal half-life and this will hopefully be included in a future notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
