{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b394606",
   "metadata": {},
   "source": [
    "# Multitask GP Regression on Molecules #\n",
    "\n",
    "An example notebook for multitask GP regression on a molecular dataset. We use a multioutput GP model, the intrinsic coregionalisation model (ICM) [1] on the Photoswitch Dataset [2] --- using a Tanimoto kernel applied to fragprint representations [2]. The paper and code for the dataset is available here:\n",
    "\n",
    "Paper: https://pubs.rsc.org/en/content/articlelanding/2022/sc/d2sc04306h\n",
    "\n",
    "Code: https://github.com/Ryan-Rhys/The-Photoswitch-Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20271807",
   "metadata": {},
   "source": [
    "## Multitask Learning with Gaussian Processes\n",
    "\n",
    "Multitask learning is concerned with using a shared representation to learn several tasks; the idea being that predictive performance on a given task may benefit from the training signals of related tasks. Multioutput Gaussian processes (MOGPs) is the term given to models that perform multitask learning in the Gaussian process framework. \n",
    "\n",
    "Formally, we seek to carry out Bayesian inference over a stochastic function $f: \\mathbb{R}^D \\to \\mathbb{R}^P$ where $P$ is the number of tasks and we have access to observations $\\{(\\mathbf{x_{11}}, y_{11}), \\dotsc , (\\mathbf{x_{1N}}, y_{1N}), \\dotsc , (\\mathbf{x_{P1}}, y_{P1}), \\dotsc , (\\mathbf{x_{PN}}, y_{PN})\\}$. For each input, we may only have labels for a subset of the tasks.\n",
    "\n",
    "To build a MOGP we compute a kernel $k(\\mathbf{x}, \\mathbf{x'}) \\cdot B[i, j]$ where $B$ is a positive semidefinite $P \\times P$ matrix , where the $(i, j)\\text{th}$ entry of the matrix $B$ multiplies the covariance of the $i$-th function at $\\mathbf{x}$ and the $j$-th function at $\\mathbf{x'}$. $B$ is often referred to as an index kernel because it indexes the tasks. \n",
    "\n",
    "Inference proceeds in analogous fashion to vanilla Gaussian processes by substituting the new expression for the kernel into the equations for the predictive mean and variance. \n",
    "\n",
    "Positive semi-definiteness of $B$ is guaranteed by parametrising the Cholesky decomposition $LL^{\\top}$ where $L$, the Cholesky factor, is a lower triangular matrix and the parameters may be learned alongside the kernel hyperparameters by optimising the marginal likelihood.\n",
    "\n",
    "An example of what correlated tasks for continuous input spaces might look like is provided below. Data taken from the GPflow tutorial (https://gpflow.readthedocs.io/en/v1.5.1-docs/notebooks/advanced/coregionalisation.html).\n",
    "\n",
    "<p align=\"center\">\n",
    "  <a>\n",
    "    <img src=\"assets/gpflow_mogp_data.png\" width=\"35%\" />\n",
    "  </a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6de1fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Turn off Graphein warnings\n",
    "\n",
    "from botorch import fit_gpytorch_model\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "\n",
    "from gauche.dataloader import MolPropLoader\n",
    "from gauche.dataloader.data_utils import transform_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5d89e8",
   "metadata": {},
   "source": [
    "We define our model. See\n",
    "\n",
    "https://docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/Multitask_GP_Regression.html\n",
    "\n",
    "for a tutorial for the use of the base multioutput GP on non-molecular data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71bd1b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our MOGP model using the Tanimoto kernel\n",
    "\n",
    "from gauche.kernels.fingerprint_kernels.tanimoto_kernel import TanimotoKernel\n",
    "\n",
    "num_tasks = 4 # number of tasks i.e. labels\n",
    "rank = 1 # increasing the rank hyperparameter allows the model to learn more expressive\n",
    "         # correlations between objectives at the expense of increasing the number of \n",
    "         # model hyperparameters and potentially overfitting.\n",
    "    \n",
    "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = TanimotoKernel()\n",
    "\n",
    "        # We learn an IndexKernel for 4 tasks\n",
    "        # (so we'll actually learn 4x4=16 tasks with correlations)\n",
    "        self.task_covar_module = gpytorch.kernels.IndexKernel(num_tasks=4, rank=1)\n",
    "\n",
    "    def forward(self, x, i):\n",
    "        mean_x = self.mean_module(x)\n",
    "\n",
    "        # Get input-input covariance\n",
    "        covar_x = self.covar_module(x)\n",
    "        # Get task-task covariance\n",
    "        covar_i = self.task_covar_module(i)\n",
    "        # Multiply the two together to get the covariance we want\n",
    "        covar = covar_x.mul(covar_i)\n",
    "        \n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9434829",
   "metadata": {},
   "source": [
    "We define our experiment parameters. In this case we are reproducing the results of the multioutput GP prediction task from https://pubs.rsc.org/en/content/articlelanding/2022/sc/d2sc04306h using 20 random splits in the ratio 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f91b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression experiment parameters, number of random splits and train/test split size\n",
    "\n",
    "n_trials = 20\n",
    "test_set_size = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32f1340",
   "metadata": {},
   "source": [
    "Load the Photoswitch Dataset via the MolPropLoader class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7037f6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 invalid labels [nan nan nan nan nan nan nan nan nan nan nan nan nan] at indices [41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 158]\n",
      "To turn validation off, use dataloader.read_csv(..., validate=False).\n",
      "Found 13 invalid labels [nan nan nan nan nan nan nan nan nan nan nan nan nan] at indices [41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 158]\n",
      "To turn validation off, use dataloader.read_csv(..., validate=False).\n",
      "Found 13 invalid labels [nan nan nan nan nan nan nan nan nan nan nan nan nan] at indices [41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 158]\n",
      "To turn validation off, use dataloader.read_csv(..., validate=False).\n",
      "Found 13 invalid labels [nan nan nan nan nan nan nan nan nan nan nan nan nan] at indices [41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 158]\n",
      "To turn validation off, use dataloader.read_csv(..., validate=False).\n"
     ]
    }
   ],
   "source": [
    "# Load the Photoswitch dataset\n",
    "\n",
    "loader = MolPropLoader()\n",
    "\n",
    "# Define a utility function for dataloading\n",
    "\n",
    "def load_task_data(task,\n",
    "                   loader=MolPropLoader(), \n",
    "                   path='Photoswitch', \n",
    "                   representation='ecfp_fragprints'):\n",
    "    \"\"\"Load data for a given task.\n",
    "    \n",
    "    Args:\n",
    "        task: str specifying the task to load data for. \n",
    "        One of ['Photoswitch', 'Photoswitch_E_n_pi', 'Photoswitch_Z_pi_pi', 'Photoswitch_Z_n_pi']\n",
    "        loader: DataLoader object\n",
    "        path: str specifying dataset.\n",
    "        representation: str specifying representation. One of ['ecfp_fingerprints', 'ecfp_fragprints', 'fragments']\n",
    "\n",
    "    Returns:\n",
    "        X_task: tensor of features for task\n",
    "        y_task: tensor of labels for task\n",
    "    \"\"\"\n",
    "    \n",
    "    if representation not in ['ecfp_fragprints', 'ecfp_fingerprints', 'fragments']:\n",
    "        raise ValueError('representation not valid.'\n",
    "                         'Please choose one of ecfp_fragprints, ecfp_fingerprints, fragments')\n",
    "                         \n",
    "    if task not in ['Photoswitch', 'Photoswitch_E_n_pi', 'Photoswitch_Z_pi_pi', 'Photoswitch_Z_n_pi']:\n",
    "        raise ValueError('task not valid. Please choose one of Photoswitch,' \n",
    "                         'Photoswitch_E_n_pi, Photoswitch_Z_pi_pi, Photoswitch_Z_n_pi')      \n",
    "    \n",
    "    loader.load_benchmark(path)\n",
    "    \n",
    "    # Featurise the molecules.\n",
    "    # We use the fragprints representations (a concatenation of Morgan fingerprints and RDKit fragment features)\n",
    "\n",
    "    loader.featurize(representation)\n",
    "    X_task = torch.from_numpy(loader.features)\n",
    "    y_task = torch.from_numpy(loader.labels)\n",
    "                         \n",
    "    return X_task, y_task\n",
    "\n",
    "# Load features X1-X4 and properties (tasks) y1-y4.\n",
    "\n",
    "X1, y1 = load_task_data('Photoswitch')\n",
    "X2, y2 = load_task_data('Photoswitch_E_n_pi')\n",
    "X3, y3 = load_task_data('Photoswitch_Z_pi_pi')\n",
    "X4, y4 = load_task_data('Photoswitch_Z_n_pi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6baed77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginning training loop...\n",
      "Starting trial 0\n",
      "19.47147948595366\n",
      "Starting trial 1\n",
      "23.791256668510048\n",
      "Starting trial 2\n",
      "26.206456838126353\n",
      "Starting trial 3\n",
      "20.030068524271805\n",
      "Starting trial 4\n",
      "32.67818405073743\n",
      "Starting trial 5\n",
      "23.453557811764345\n",
      "Starting trial 6\n",
      "20.75033301782167\n",
      "Starting trial 7\n",
      "23.209958101999543\n",
      "Starting trial 8\n",
      "24.654899652465076\n",
      "Starting trial 9\n",
      "17.65168764479574\n",
      "Starting trial 10\n",
      "23.086582644605222\n",
      "Starting trial 11\n",
      "16.02576800479441\n",
      "Starting trial 12\n",
      "24.76230610058969\n",
      "Starting trial 13\n",
      "18.59783949898034\n",
      "Starting trial 14\n",
      "23.796203872804114\n",
      "Starting trial 15\n",
      "18.30003157481471\n",
      "Starting trial 16\n",
      "22.394982557795256\n",
      "Starting trial 17\n",
      "23.632087011071746\n",
      "Starting trial 18\n",
      "23.739771849691213\n",
      "Starting trial 19\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Helper function for model evaluation.\n",
    "\"\"\"\n",
    "\n",
    "def prevent_test_leakage(x1, x2, x3, y1, y2, y3, X_test):\n",
    "    \"\"\"\n",
    "    Function to prevent test leakage in train/test splits for multitask learning, for example,\n",
    "    for test data point x_i, do not provide the model with auxiliary tasks P2-P4 when predicting P1.\n",
    "\n",
    "    param: x1, x2, x3: input molecules for other tasks\n",
    "    param: y1, y2, y3: labels for other tasks\n",
    "    param: X_test: the test molecules\n",
    "    \"\"\"\n",
    "\n",
    "    other_tasks = [x1, x2, x3]\n",
    "    other_labels = [y1, y2, y3]\n",
    "    for i in range(len(other_tasks)):\n",
    "        indices_to_delete = []\n",
    "        for j in range(len(other_tasks[i])):\n",
    "            other_mol = other_tasks[i][j]\n",
    "            if np.any([np.array_equal(other_mol, mol) for mol in X_test]) == True:\n",
    "                indices_to_delete.append(j)\n",
    "        indices_to_delete.reverse()\n",
    "        for index in indices_to_delete:\n",
    "            other_tasks[i] = np.delete(other_tasks[i], index, axis=0)\n",
    "            other_labels[i] = np.delete(other_labels[i], index, axis=0)\n",
    "\n",
    "    x1, x2, x3 = other_tasks[0], other_tasks[1], other_tasks[2]\n",
    "    y1, y2, y3 = other_labels[0], other_labels[1], other_labels[2]\n",
    "\n",
    "    return x1, x2, x3, y1, y2, y3\n",
    "\n",
    "# Experiment parameters, train/test split and task to run prediction for\n",
    "test_set_size = 0.2\n",
    "task = 'e_iso_pi'\n",
    "\n",
    "r2_list = []\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "\n",
    "print('\\nBeginning training loop...')\n",
    "\n",
    "for i in range(0, n_trials):\n",
    "    \n",
    "    print(f'Starting trial {i}')\n",
    "                \n",
    "    if task == 'e_iso_pi':\n",
    "        X_task = X1\n",
    "        y_task = y1\n",
    "    elif task == 'z_iso_pi':\n",
    "        X_task = X2\n",
    "        y_task = y2\n",
    "    elif task == 'e_iso_n':\n",
    "        X_task = X3\n",
    "        y_task = y3\n",
    "    else:\n",
    "        X_task = X4\n",
    "        y_task = y4\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_task, y_task, test_size=test_set_size, random_state=i)\n",
    "    \n",
    "    # Partition the training data into tasks (most difficult part of training a multioutput GP!)\n",
    "\n",
    "    if task == 'e_iso_pi':\n",
    "\n",
    "        # Modify the x-values for the other tasks to exclude X_test\n",
    "        X2_new, X3_new, X4_new, y2_new, y3_new, y4_new = \\\n",
    "            prevent_test_leakage(X2, X3, X4, y2, y3, y4, X_test)\n",
    "                        \n",
    "        train_i_task1 = torch.full((X_train.shape[0], 1), dtype=torch.long, fill_value=0)\n",
    "        train_i_task2 = torch.full((X2_new.shape[0], 1), dtype=torch.long, fill_value=1)\n",
    "        train_i_task3 = torch.full((X3_new.shape[0], 1), dtype=torch.long, fill_value=2)\n",
    "        train_i_task4 = torch.full((X4_new.shape[0], 1), dtype=torch.long, fill_value=3)\n",
    "        \n",
    "        full_train_x = torch.cat([X_train, X2_new, X3_new, X4_new])\n",
    "        full_train_y = torch.cat([y_train, y2_new, y3_new, y4_new]).flatten()\n",
    "        \n",
    "        test_i_task = torch.full((X_test.shape[0], 1), dtype=torch.long, fill_value=0)\n",
    "\n",
    "\n",
    "    elif task == 'e_iso_n':\n",
    "        X1, X3, X4, y1, y3, y4 = \\\n",
    "            prevent_test_leakage(X1, X3, X4, y1, y3, y4, X_test)\n",
    "        \n",
    "        train_i_task1 = torch.full((X1.shape[0], 1), dtype=torch.long, fill_value=0)\n",
    "        train_i_task2 = torch.full((X_train.shape[0], 1), dtype=torch.long, fill_value=1)\n",
    "        train_i_task3 = torch.full((X3.shape[0], 1), dtype=torch.long, fill_value=2)\n",
    "        train_i_task4 = torch.full((X4.shape[0], 1), dtype=torch.long, fill_value=3)\n",
    "\n",
    "        full_train_x = torch.cat([X1, X_train, X3, X4])\n",
    "        full_train_y = torch.cat([y1, y_train, y3, y4])\n",
    "        \n",
    "        test_i_task = torch.full((X_test.shape[0], 1), dtype=torch.long, fill_value=1)\n",
    "\n",
    "\n",
    "    elif task == 'z_iso_pi':\n",
    "        X1, X2, X4, y1, y2, y4 = \\\n",
    "            prevent_test_leakage(X1, X2, X4, y1, y2, y4, X_test)\n",
    "        \n",
    "        train_i_task1 = torch.full((X1.shape[0], 1), dtype=torch.long, fill_value=0)\n",
    "        train_i_task2 = torch.full((X2.shape[0], 1), dtype=torch.long, fill_value=1)\n",
    "        train_i_task3 = torch.full((X_train.shape[0], 1), dtype=torch.long, fill_value=2)\n",
    "        train_i_task4 = torch.full((X4.shape[0], 1), dtype=torch.long, fill_value=3)\n",
    "        \n",
    "        full_train_x = torch.cat([X1, X2, X_train, X4])\n",
    "        full_train_y = torch.cat([y1, y2, y_train, y4])\n",
    "        \n",
    "        test_i_task = torch.full((X_test.shape[0], 1), dtype=torch.long, fill_value=2)\n",
    "\n",
    "\n",
    "    else:\n",
    "        X1, X2, X3, y1, y2, y3 = \\\n",
    "            prevent_test_leakage(X1, X2, X3, y1, y2, y3, X_test)\n",
    "        \n",
    "        train_i_task1 = torch.full((X1.shape[0], 1), dtype=torch.long, fill_value=0)\n",
    "        train_i_task2 = torch.full((X2.shape[0], 1), dtype=torch.long, fill_value=1)\n",
    "        train_i_task3 = torch.full((X3.shape[0], 1), dtype=torch.long, fill_value=2)\n",
    "        train_i_task4 = torch.full((X_train.shape[0], 1), dtype=torch.long, fill_value=3)\n",
    "\n",
    "        full_train_x = torch.cat([X1, X2, X3, X_train])\n",
    "        full_train_y = torch.cat([y1, y2, y3, y_train])\n",
    "        \n",
    "        test_i_task = torch.full((X_test.shape[0], 1), dtype=torch.long, fill_value=3)\n",
    "\n",
    "\n",
    "    full_train_i = torch.cat([train_i_task1, train_i_task2, train_i_task3, train_i_task4])\n",
    "\n",
    "    # Gaussian likelihood\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    \n",
    "    # Here we have two items that we're passing in as train_inputs\n",
    "    model = MultitaskGPModel((full_train_x.float(), full_train_i.float()), full_train_y.float(), likelihood)\n",
    "    \n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    # Use the BoTorch utility for fitting GPs in order to use the LBFGS-B optimiser (recommended)\n",
    "    # Set the jitter level larger than the default for the MOGP\n",
    "    with gpytorch.settings.cholesky_jitter(1e-3):\n",
    "        fit_gpytorch_model(mll)\n",
    "    \n",
    "    # Get into evaluation (predictive posterior) mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # The gpytorch.settings.fast_pred_var flag activates LOVE (for fast variances)\n",
    "    # See https://arxiv.org/abs/1803.06058\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred_y = likelihood(model(X_test.float(), test_i_task.float()))\n",
    "\n",
    "    y_pred = observed_pred_y.mean\n",
    "\n",
    "    # Compute R^2, RMSE and MAE on Test set\n",
    "    score = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    print(rmse)\n",
    "\n",
    "    r2_list.append(score)\n",
    "    rmse_list.append(rmse)\n",
    "    mae_list.append(mae)\n",
    "        \n",
    "r2_list = np.array(r2_list)\n",
    "rmse_list = np.array(rmse_list)\n",
    "mae_list = np.array(mae_list)\n",
    "\n",
    "print(\"\\nmean R^2: {:.4f} +- {:.4f}\".format(np.mean(r2_list), np.std(r2_list)/np.sqrt(len(r2_list))))\n",
    "print(\"mean RMSE: {:.4f} +- {:.4f}\".format(np.mean(rmse_list), np.std(rmse_list)/np.sqrt(len(rmse_list))))\n",
    "print(\"mean MAE: {:.4f} +- {:.4f}\\n\".format(np.mean(mae_list), np.std(mae_list)/np.sqrt(len(mae_list)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ae93ed",
   "metadata": {},
   "source": [
    "Multitask learning is especially powerful when there are correlations between different tasks as in the case of photoswitch transition wavelengths. Additionally, when seeking to predict the properties of a molecule for which correlated task labels are available, the multioutput Gaussian process can leverage this information to inform its predictions. For more on multitask learning on molecules cf Ramsundar et al. [3]. For more on multioutput Gaussian processes see [4]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75072e66",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Bonilla, E.V., Chai, K. and Williams, C., [Multi-task Gaussian process prediction](https://proceedings.neurips.cc/paper/2007/hash/66368270ffd51418ec58bd793f2d9b1b-Abstract.html). Advances in Neural Information Processing Systems, 20, 2007.\n",
    "\n",
    "[2] Griffiths, R.R., Greenfield, J.L., Thawani, A.R., Jamasb, A.R., Moss, H.B., Bourached, A., Jones, P., McCorkindale, W., Aldrick, A.A. and Fuchter, M.J., 2022. [Data-driven discovery of molecular photoswitches with multioutput Gaussian processes](https://pubs.rsc.org/en/content/articlehtml/2022/sc/d2sc04306h). Chemical Science.\n",
    "\n",
    "[3] Ramsundar, B., Kearnes, S., Riley, P., Webster, D., Konerding, D. and Pande, V., 2015. [Massively multitask networks for drug discovery](https://arxiv.org/abs/1502.02072). arXiv preprint arXiv:1502.02072.\n",
    "\n",
    "[4] [Gaussian Processes: from one to many outputs\n",
    "](https://invenia.github.io/blog/2021/02/19/OILMM-pt1/), Invenia Blog.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
